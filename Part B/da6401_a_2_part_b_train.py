# -*- coding: utf-8 -*-
"""DA6401_A-2_Part-B

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/sajidma23m013/da6401-a-2-part-b.cb382e0d-95a2-43f3-83a8-1b7bbf9c86c6.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250419/auto/storage/goog4_request%26X-Goog-Date%3D20250419T153947Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D44b37fd98c290a25e93a9084dc6de9a3d3d20496e301d4d23469d42586698e5e0e33dc4131b6e61a8cde863e6db4f3581e9d02d6bda7f25cde857a25b5c9d2c765b29f08cbf2127848447f05fa9b098e3ddf6b883547ec53a5b3aa4793b2fffba146e2ef9d8ced45a79eddd18ae753605479c43595f3f85b57a253f16773c518dafc22f80f2a854deec0622e9ac0e38b7aa30cfaf9efff6ae1675a2ddefe99bd14f6c6203f5cd22cca96a94e5bef122345bf3385e30c22113747f5457b1febb635967968660a4507609b3d92690e409ed988a983d3d39fff13aca355ca22f7963236e9f4accc27a55bfe8768365ac48261c4b592c4beea9d9ab8f122e8b2d5b4
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

sajidma23m013_inaturalist_dataset_path = kagglehub.dataset_download('sajidma23m013/inaturalist-dataset')

print('Data source import complete.')

!pip install wandb

import wandb
wandb.login(key='53b259076c07d0811d73bf26bfef7437e04dbf66')
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, SubsetRandomSampler
from sklearn.model_selection import train_test_split
import wandb

# Setup device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load pre-trained model and apply freezing
def pretrain_model(model_name, freeze_percent, freeze_all_except_last_layer):
    if model_name == 'resnet':
        model = models.resnet50(pretrained=True)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, 10)
    elif model_name == 'googlenet':
        model = models.googlenet(pretrained=True)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, 10)
    else:
        raise ValueError("Invalid model name. Choose 'resnet' or 'googlenet'.")

    total_layers = sum(1 for _ in model.children())
    num_freeze = int(total_layers * freeze_percent)

    if freeze_all_except_last_layer == 'Yes':
        for name, param in model.named_parameters():
            if not name.startswith('fc'):
                param.requires_grad = False
    else:
        count = 0
        for name, child in model.named_children():
            for param in child.parameters():
                param.requires_grad = count >= num_freeze
            count += 1

    return model

# Data loading and augmentation
def load_dataset(data_directory, apply_augmentation):
    if apply_augmentation == 'Yes':
        transform_pipeline = transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
            transforms.RandomRotation(20),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
    else:
        transform_pipeline = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

    dataset = ImageFolder(root=data_directory, transform=transform_pipeline)
    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)
    train_loader = DataLoader(dataset, batch_size=32, sampler=SubsetRandomSampler(train_idx))
    val_loader = DataLoader(dataset, batch_size=32, sampler=SubsetRandomSampler(val_idx))
    return train_loader, val_loader

# Training logic
def train_model_on_data(model, train_loader):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    model.train()

    total_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return total_loss / len(train_loader), 100 * correct / total

def evaluate_model_on_validation(model, val_loader):
    criterion = nn.CrossEntropyLoss()
    model.eval()

    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return val_loss / len(val_loader), 100 * correct / total

def train_and_validate_model(model, train_loader, val_loader, num_epochs):
    for epoch in range(num_epochs):
        train_loss, train_acc = train_model_on_data(model, train_loader)
        val_loss, val_acc = evaluate_model_on_validation(model, val_loader)

        print(f'Epoch {epoch + 1}/{num_epochs}, '
              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

        wandb.log({
            'Epoch': epoch + 1,
            'Train Loss': train_loss,
            'Train Accuracy': train_acc,
            'Validation Loss': val_loss,
            'Validation Accuracy': val_acc
        })

    print("Training complete!")

# CLI entry point
def main():
    parser = argparse.ArgumentParser(description="Transfer Learning Trainer")

    parser.add_argument('--data_dir', type=str, required=True, help='Path to dataset directory')
    parser.add_argument('--model_name', type=str, choices=['resnet', 'googlenet'], default='resnet', help='Pretrained model to use')
    parser.add_argument('--freeze_percent', type=float, default=0.25, help='Percentage of layers to freeze (0.0 to 1.0)')
    parser.add_argument('--freeze_all_except_last_layer', type=str, choices=['Yes', 'No'], default='No', help='Freeze all layers except last?')
    parser.add_argument('--epochs', type=int, default=5, help='Number of training epochs')
    parser.add_argument('--apply_augmentation', type=str, choices=['Yes', 'No'], default='No', help='Apply data augmentation')

    args = parser.parse_args()

    wandb.init(project="DA6401_A-2_Part-B", config=vars(args))
    config = wandb.config

    model = pretrain_model(config.model_name, config.freeze_percent, config.freeze_all_except_last_layer)
    model = model.to(device)

    train_loader, val_loader = load_dataset(config.data_dir, config.apply_augmentation)

    train_and_validate_model(model, train_loader, val_loader, config.epochs)

    wandb.finish()

if __name__ == '__main__':
    main()
