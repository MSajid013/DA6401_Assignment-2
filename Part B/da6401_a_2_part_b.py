# -*- coding: utf-8 -*-
"""DA6401_A-2_Part-B

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/sajidma23m013/da6401-a-2-part-b.b50e16d3-6138-4be3-8902-1b4a16827031.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250419/auto/storage/goog4_request%26X-Goog-Date%3D20250419T062639Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D220be20b3aa72cae3718ffc937ed74b391fdec63f08d600c3684f24ed57f0f888463af4c576179ce80c8ac66ba41b6f631b386e469947d716a4b9119a372cad74dc0d27b3de228662b1e1fd493dd1f87195535895561077c70463d0558cadffb192276b2e7083453b36fdd49bd25422dd7923adc04b2525f77a9f2f1939ea72f22cc0a8e2db60fea5515e28d0b04737e08ae34926bca02eb06ba672d6fe0b33b0e5b565379571276425749b9a7b5adc6341001f5bc6c68d95c74c74b2334d9a0c2e94fbce9221b99277d6a04c4f2e6321113b70dc2a5ae6707ad499548e0c06f53c1b6e61839bbdcbde69508205f0fe9e8e2cb9a779045680d626d7ab6dedbdf
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

sajidma23m013_inaturalist_dataset_path = kagglehub.dataset_download('sajidma23m013/inaturalist-dataset')

print('Data source import complete.')

!pip install wandb

import wandb
wandb.login(key='53b259076c07d0811d73bf26bfef7437e04dbf66')

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
from sklearn.model_selection import train_test_split
from torchvision import models

# Check if CUDA is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define function for pretraining a model
def pretrain_model(model_name, freeze_percent, freeze_all_except_last_layer):
    '''
    This function initializes a pre-trained model based on the model_name argument,
    freezes layers based on the freeze_percent, and optionally freezes all layers except
    the last fully connected layer.

    Parameters:
    - model_name (str): The name of the model to load (e.g., 'resnet', 'googlenet')
    - freeze_percent (float): Percentage of layers to freeze (e.g., 0.25 for 25%)
    - freeze_all_except_last_layer (str): If 'Yes', only the last layer is trainable. If 'No', freeze layers based on freeze_percent.

    Returns:
    - pretrained_model (torch.nn.Module): The model with modified layers and frozen parameters.
    '''

    # Load pre-trained model based on the provided model name
    if model_name == 'resnet':
        pretrained_model = models.resnet50(pretrained=True)
    elif model_name == 'googlenet':
        pretrained_model = models.googlenet(pretrained=True)

    # Modify the final fully connected layer to match the number of classes (10 in this case)
    nodes_fc = pretrained_model.fc.in_features  # Get the input features of the last fully connected layer
    pretrained_model.fc = nn.Linear(nodes_fc, 10)  # Change to 10 classes (modify if necessary)

    # Calculate the number of layers to freeze based on the freeze_percent
    total_layers = sum(1 for _ in pretrained_model.children())
    num_freeze = int(total_layers * freeze_percent)

    # Freeze layers based on the 'freeze_all_except_last_layer' flag
    if freeze_all_except_last_layer == 'Yes':
        for name, parameter in pretrained_model.named_parameters():
            if not name.startswith('fc'):  # Exclude the last layer named 'fc'
                parameter.requires_grad = False  # Freeze this layer

    if freeze_all_except_last_layer == 'No':
        count = 0
        # Freeze layers based on the 'freeze_percent' argument
        for name, child in pretrained_model.named_children():
            if count < num_freeze:
                for parameter in child.parameters():
                    parameter.requires_grad = False  # Freeze the layer
            else:
                for parameter in child.parameters():
                    parameter.requires_grad = True  # Make the layer trainable
            count += 1

    return pretrained_model

# Example usage
freeze_percent = 0.25  # Freeze 25% of layers
freeze_all_except_last_layer = 'No'
model_name = 'googlenet'  # Using GoogLeNet
model = pretrain_model(model_name, freeze_percent, freeze_all_except_last_layer)
print(model)  # Print the model architecture

# Example usage
freeze_percent = 0.25  # Freeze 25% of layers
freeze_all_except_last_layer = 'No'
model_name = 'resnet'  # Using ResNet
model = pretrain_model(model_name, freeze_percent, freeze_all_except_last_layer)
print(model)  # Print the model architecture

from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, SubsetRandomSampler
from sklearn.model_selection import train_test_split

def load_dataset(data_directory, apply_augmentation):
    """
    Load and preprocess image dataset with optional data augmentation.

    Args:
        data_directory (str): Path to the dataset directory.
        apply_augmentation (str): 'Yes' to apply data augmentation, 'No' otherwise.

    Returns:
        train_loader (DataLoader): DataLoader for the training set.
        val_loader (DataLoader): DataLoader for the validation set.
    """

    # Define image transformation pipeline
    if apply_augmentation == 'Yes':
        transform_pipeline = transforms.Compose([
            transforms.RandomResizedCrop(224),                       # Randomly crop and resize to 224x224
            transforms.RandomHorizontalFlip(),                       # Random horizontal flip
            transforms.ColorJitter(brightness=0.2,
                                   contrast=0.2,
                                   saturation=0.2,
                                   hue=0.1),                         # Random brightness, contrast, etc.
            transforms.RandomRotation(20),                           # Random rotation up to 20 degrees
            transforms.ToTensor(),                                   # Convert image to tensor
            transforms.Normalize((0.5, 0.5, 0.5),
                                 (0.5, 0.5, 0.5))                    # Normalize to [-1, 1] range
        ])
    else:
        transform_pipeline = transforms.Compose([
            transforms.Resize((224, 224)),                           # Resize to 224x224
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5),
                                 (0.5, 0.5, 0.5))
        ])

    # Load dataset from directory
    dataset = ImageFolder(root=data_directory, transform=transform_pipeline)

    # Split indices for training and validation sets
    train_indices, val_indices = train_test_split(
        list(range(len(dataset))), test_size=0.2, random_state=42
    )

    # Create samplers for training and validation
    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    # DataLoaders with batch size of 32
    train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)
    val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)

    return train_loader, val_loader

import torch
import torch.nn as nn
import torch.optim as optim

def train_model_on_data(model, train_loader):
    """
    Train the given model on training data for one epoch.

    Args:
        model (nn.Module): The neural network to train.
        train_loader (DataLoader): DataLoader for the training set.

    Returns:
        avg_loss (float): Average training loss over the epoch.
        train_accuracy (float): Training accuracy as a percentage.
    """

    # Define loss function and optimizer
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Set model to training mode
    model.train()

    # Initialize counters for tracking performance
    cumulative_loss = 0.0
    total_correct = 0
    total_samples = 0

    # Training loop over batches
    for batch_images, batch_labels in train_loader:
        # Move data to device (GPU if available)
        batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)

        # Forward pass
        predictions = model(batch_images)
        loss = loss_function(predictions, batch_labels)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Accumulate loss
        cumulative_loss += loss.item()

        # Compute number of correct predictions
        _, predicted_classes = torch.max(predictions, 1)
        total_correct += (predicted_classes == batch_labels).sum().item()
        total_samples += batch_labels.size(0)

    # Compute average loss and accuracy
    avg_loss = cumulative_loss / len(train_loader)
    train_accuracy = 100 * total_correct / total_samples

    return avg_loss, train_accuracy

import torch
import torch.nn as nn

def evaluate_model_on_validation(model, validation_loader):
    """
    Evaluate the model on the validation set.

    Args:
        model (nn.Module): The trained model.
        validation_loader (DataLoader): DataLoader for the validation set.

    Returns:
        avg_val_loss (float): Average loss on the validation set.
        validation_accuracy (float): Accuracy on the validation set as a percentage.
    """

    # Set model to evaluation mode (disables dropout, batchnorm updates, etc.)
    model.eval()

    # Define loss function
    loss_function = nn.CrossEntropyLoss()

    # Initialize counters for loss and accuracy
    cumulative_val_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    # Disable gradient computation during evaluation
    with torch.no_grad():
        for batch_images, batch_labels in validation_loader:
            # Move data to GPU if available
            batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)

            # Forward pass
            outputs = model(batch_images)
            loss = loss_function(outputs, batch_labels)
            cumulative_val_loss += loss.item()

            # Calculate accuracy
            _, predicted_classes = torch.max(outputs, 1)
            correct_predictions += (predicted_classes == batch_labels).sum().item()
            total_samples += batch_labels.size(0)

    # Compute average loss and accuracy
    avg_val_loss = cumulative_val_loss / len(validation_loader)
    validation_accuracy = 100 * correct_predictions / total_samples

    return avg_val_loss, validation_accuracy

# Function to train and validate the model
def train_and_validate_model(model, train_loader, val_loader, num_epochs):
    """
    Trains the given model and evaluates on the validation set for each epoch.

    Args:
        model (nn.Module): The neural network to train.
        train_loader (DataLoader): DataLoader for training data.
        val_loader (DataLoader): DataLoader for validation data.
        num_epochs (int): Number of training epochs.
    """

    # Define the loss function and optimizer
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(num_epochs):
        # Training phase
        avg_train_loss, train_accuracy = train_model_on_data(model, train_loader)
        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')

        # Log training metrics to wandb
        wandb.log({'Train Loss': avg_train_loss})
        wandb.log({'Train Accuracy': train_accuracy})

        # Validation phase
        avg_val_loss, val_accuracy = evaluate_model_on_validation(model, val_loader)
        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')

        # Log validation metrics to wandb
        wandb.log({'Validation Loss': avg_val_loss})
        wandb.log({'Validation Accuracy': val_accuracy})
        wandb.log({'Epoch': epoch + 1})

    print('Training complete!')

# Sweep configuration dictionary
sweep_config = {
    'method': 'bayes',  # 'bayes' will use Bayesian optimization to find optimal hyperparameters, other options are 'grid' or 'random'

    'metric': {
        'name': 'Validation Accuracy',  # Metric to optimize during the sweep
        'goal': 'maximize'  # Goal is to maximize the validation accuracy
    },

    'parameters': {

        'freeze_percent': {
            'values': [0.0, 0.25, 0.5, 0.75]  # Different percentages of layers to freeze during fine-tuning
        },

        'model_name': {
            'values': ['resnet', 'googlenet']  # Pre-trained model options for fine-tuning
        },

        'freeze_all_except_last_layer': {
            'values': ['Yes', 'No']  # Option to freeze all layers except the last one or not
        },

        'epochs': {
            'value': 5  # Number of epochs for each training run
        }
    }
}

# Initialize the W&B sweep using the config
sweep_id = wandb.sweep(sweep_config, project="DA6401_A-2_Part-B")  # Initialize sweep in the project "DA6401_A-2_Part-B"

def main():
    '''
    WandB calls the main function each time with a different combination of hyperparameters.
    We can retrieve the configuration values from `wandb.config` and use them in our model training.

    This function initializes the W&B run, loads the pre-trained model with specified hyperparameters,
    prepares the dataset, and starts the training process.
    '''

    # Initialize W&B run
    with wandb.init() as run:
        # Define the run name based on hyperparameters for better tracking
        run_name = "ep" + str(wandb.config.epochs) + "_fp-" + str(wandb.config.freeze_percent) + "_model-" + str(wandb.config.model_name)
        wandb.run.name = run_name  # Set the name for this run in W&B

        # Load the pre-trained model based on configuration
        model = pretrain_model(
            model_name=wandb.config.model_name,
            freeze_percent=wandb.config.freeze_percent,
            freeze_all_except_last_layer=wandb.config.freeze_all_except_last_layer
        )

        # Move the model to the appropriate device (CPU or GPU)
        model = model.to(device)

        # Define the directory where the training data is located
        data_dir = '/kaggle/input/inaturalist-dataset/inaturalist_12K/train'

        # Load the training and validation data with or without data augmentation
        train_data, val_data = load_dataset(data_dir, apply_augmentation='No')

        # Train the model with the loaded data and configurations
        train_and_validate_model(model, train_data, val_data, num_epochs=wandb.config.epochs)


# Start the W&B sweep, calling the main function for the specified number of times
wandb.agent(sweep_id, function=main, count=10)  # Number of runs = 1 in this case, but can be increased
wandb.finish()  # Finish the W&B run after the sweep is complete

# Sweep configuration dictionary
sweep_config = {
    'method': 'bayes',  # 'bayes' will use Bayesian optimization to find optimal hyperparameters, other options are 'grid' or 'random'

    'metric': {
        'name': 'Validation Accuracy',  # Metric to optimize during the sweep
        'goal': 'maximize'  # Goal is to maximize the validation accuracy
    },

    'parameters': {

        'freeze_percent': {
            'values': [0.75]  # Different percentages of layers to freeze during fine-tuning
        },

        'model_name': {
            'values': ['resnet']  # Pre-trained model options for fine-tuning
        },

        'freeze_all_except_last_layer': {
            'values': ['Yes']  # Option to freeze all layers except the last one or not
        },

        'epochs': {
            'value': 6  # Number of epochs for each training run
        }
    }
}

# Initialize the W&B sweep using the config
sweep_id = wandb.sweep(sweep_config, project="DA6401_A-2_Part-B")  # Initialize sweep in the project "DA6401_A-2_Part-B"

def main():
    '''
    WandB calls the main function each time with a different combination of hyperparameters.
    We can retrieve the configuration values from `wandb.config` and use them in our model training.

    This function initializes the W&B run, loads the pre-trained model with specified hyperparameters,
    prepares the dataset, and starts the training process.
    '''

    # Initialize W&B run
    with wandb.init() as run:
        # Define the run name based on hyperparameters for better tracking
        run_name = "ep" + str(wandb.config.epochs) + "_fp-" + str(wandb.config.freeze_percent) + "_model-" + str(wandb.config.model_name)
        wandb.run.name = run_name  # Set the name for this run in W&B

        # Load the pre-trained model based on configuration
        model = pretrain_model(
            model_name=wandb.config.model_name,
            freeze_percent=wandb.config.freeze_percent,
            freeze_all_except_last_layer=wandb.config.freeze_all_except_last_layer
        )

        # Move the model to the appropriate device (CPU or GPU)
        model = model.to(device)

        # Define the directory where the training data is located
        data_dir = '/kaggle/input/inaturalist-dataset/inaturalist_12K/train'

        # Load the training and validation data with or without data augmentation
        train_data, val_data = load_dataset(data_dir, apply_augmentation='No')

        # Train the model with the loaded data and configurations
        train_and_validate_model(model, train_data, val_data, num_epochs=wandb.config.epochs)


# Start the W&B sweep, calling the main function for the specified number of times
wandb.agent(sweep_id, function=main, count=10)  # Number of runs = 10 in this case, but can be increased
wandb.finish()  # Finish the W&B run after the sweep is complete

